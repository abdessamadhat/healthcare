# EDA_Quality.py - Analyse Exploratoire des Donn√©es D√©taill√©e
# Analyse compl√®te du dataset Quality.csv pour pr√©diction de la qualit√© des soins

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from scipy import stats
from scipy.stats import chi2_contingency, pearsonr, spearmanr
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff

# Configuration
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Configuration pour les graphiques
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11

print("="*80)
print("üè• ANALYSE EXPLORATOIRE DES DONN√âES - QUALITY HEALTHCARE DATASET")
print("="*80)

def load_and_basic_info():
    """Chargement des donn√©es et informations de base"""
    print("\nüìä 1. CHARGEMENT ET INFORMATIONS G√âN√âRALES")
    print("-" * 50)
    
    # Chargement
    df = pd.read_csv("Quality.csv")
    
    print(f"üìà Dimensions du dataset: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
    print(f"üìä Taille en m√©moire: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")
    
    print("\nüîç Aper√ßu des premi√®res lignes:")
    print(df.head())
    
    print("\nüîç Aper√ßu des derni√®res lignes:")
    print(df.tail())
    
    print("\nüìã Informations sur les colonnes:")
    print(df.info())
    
    print("\nüìä Types de donn√©es:")
    print(df.dtypes)
    
    return df

def missing_values_analysis(df):
    """Analyse des valeurs manquantes"""
    print("\nüîç 2. ANALYSE DES VALEURS MANQUANTES")
    print("-" * 50)
    
    missing_data = df.isnull().sum()
    missing_percent = (df.isnull().sum() / len(df)) * 100
    
    missing_df = pd.DataFrame({
        'Colonnes': df.columns,
        'Valeurs manquantes': missing_data,
        'Pourcentage (%)': missing_percent
    }).sort_values('Valeurs manquantes', ascending=False)
    
    print(missing_df)
    
    if missing_data.sum() == 0:
        print("‚úÖ Aucune valeur manquante d√©tect√©e!")
    else:
        print(f"‚ö†Ô∏è  Total de valeurs manquantes: {missing_data.sum()}")
        
        # Visualisation des valeurs manquantes
        plt.figure(figsize=(12, 6))
        sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
        plt.title('Carte de chaleur des valeurs manquantes')
        plt.tight_layout()
        plt.show()
    
    return missing_df

def data_types_and_conversion(df):
    """Analyse et conversion des types de donn√©es"""
    print("\nüîÑ 3. ANALYSE ET CONVERSION DES TYPES DE DONN√âES")
    print("-" * 50)
    
    print("Types de donn√©es avant conversion:")
    print(df.dtypes)
    
    # Conversion de StartedOnCombination
    if df['StartedOnCombination'].dtype == 'object':
        print("\nüîÑ Conversion de StartedOnCombination (FALSE/TRUE ‚Üí 0/1)")
        df['StartedOnCombination'] = df['StartedOnCombination'].map({'FALSE': 0, 'TRUE': 1})
        print("‚úÖ Conversion r√©ussie")
    
    print("\nTypes de donn√©es apr√®s conversion:")
    print(df.dtypes)
    
    # V√©rification des valeurs uniques pour variables cat√©gorielles
    categorical_cols = ['StartedOnCombination', 'PoorCare']
    for col in categorical_cols:
        print(f"\nüè∑Ô∏è  Valeurs uniques pour {col}:")
        print(f"   Valeurs: {sorted(df[col].unique())}")
        print(f"   Comptes: {df[col].value_counts().to_dict()}")
    
    return df

def descriptive_statistics(df):
    """Statistiques descriptives d√©taill√©es"""
    print("\nüìä 4. STATISTIQUES DESCRIPTIVES D√âTAILL√âES")
    print("-" * 50)
    
    print("üìà Statistiques pour toutes les variables num√©riques:")
    desc_stats = df.describe()
    print(desc_stats)
    
    # Statistiques suppl√©mentaires
    print("\nüìä Statistiques suppl√©mentaires:")
    additional_stats = pd.DataFrame({
        'M√©diane': df.select_dtypes(include=[np.number]).median(),
        'Mode': df.select_dtypes(include=[np.number]).mode().iloc[0],
        'Variance': df.select_dtypes(include=[np.number]).var(),
        '√âcart-type': df.select_dtypes(include=[np.number]).std(),
        'Asym√©trie (Skewness)': df.select_dtypes(include=[np.number]).skew(),
        'Aplatissement (Kurtosis)': df.select_dtypes(include=[np.number]).kurtosis(),
        'Q1 (25%)': df.select_dtypes(include=[np.number]).quantile(0.25),
        'Q3 (75%)': df.select_dtypes(include=[np.number]).quantile(0.75),
        'IQR': df.select_dtypes(include=[np.number]).quantile(0.75) - df.select_dtypes(include=[np.number]).quantile(0.25)
    })
    print(additional_stats)
    
    return desc_stats, additional_stats

def target_variable_analysis(df):
    """Analyse d√©taill√©e de la variable cible"""
    print("\nüéØ 5. ANALYSE DE LA VARIABLE CIBLE (PoorCare)")
    print("-" * 50)
    
    target_counts = df['PoorCare'].value_counts()
    target_props = df['PoorCare'].value_counts(normalize=True)
    
    print("üî¢ Distribution de PoorCare:")
    for idx, (count, prop) in enumerate(zip(target_counts, target_props)):
        label = "Bonne qualit√© (0)" if idx == 0 else "Mauvaise qualit√© (1)"
        print(f"   {label}: {count} ({prop:.2%})")
    
    # Test d'√©quilibrage des classes
    ratio = target_counts.min() / target_counts.max()
    print(f"\n‚öñÔ∏è  Ratio d'√©quilibrage: {ratio:.3f}")
    
    if ratio < 0.3:
        print("‚ö†Ô∏è  Dataset d√©s√©quilibr√© d√©tect√©!")
    elif ratio < 0.5:
        print("‚ö†Ô∏è  L√©ger d√©s√©quilibre des classes")
    else:
        print("‚úÖ Classes relativement √©quilibr√©es")
    
    # Visualisation
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Graphique en barres
    target_counts.plot(kind='bar', ax=axes[0], color=['lightgreen', 'lightcoral'])
    axes[0].set_title('Distribution de PoorCare (Effectifs)')
    axes[0].set_xlabel('PoorCare')
    axes[0].set_ylabel('Nombre de patients')
    axes[0].tick_params(axis='x', rotation=0)
    
    # Graphique en secteurs
    target_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', 
                      colors=['lightgreen', 'lightcoral'])
    axes[1].set_title('Distribution de PoorCare (Pourcentages)')
    axes[1].set_ylabel('')
    
    plt.tight_layout()
    plt.show()
    
    return target_counts, target_props

def feature_distributions(df):
    """Analyse des distributions des variables explicatives"""
    print("\nüìä 6. ANALYSE DES DISTRIBUTIONS DES VARIABLES")
    print("-" * 50)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols.remove('PoorCare')  # Exclure la variable cible
    
    # Graphiques de distribution
    n_cols = 3
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))
    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes
    
    for i, col in enumerate(numeric_cols):
        if i < len(axes):
            # Histogramme avec courbe de densit√©
            axes[i].hist(df[col], bins=20, alpha=0.7, density=True, color='skyblue', edgecolor='black')
            
            # Ajout de la courbe de densit√©
            df[col].plot.density(ax=axes[i], color='red', linewidth=2)
            
            axes[i].set_title(f'Distribution de {col}')
            axes[i].set_xlabel(col)
            axes[i].set_ylabel('Densit√©')
            axes[i].grid(True, alpha=0.3)
            
            # Statistiques sur le graphique
            mean_val = df[col].mean()
            median_val = df[col].median()
            axes[i].axvline(mean_val, color='red', linestyle='--', label=f'Moyenne: {mean_val:.2f}')
            axes[i].axvline(median_val, color='green', linestyle='--', label=f'M√©diane: {median_val:.2f}')
            axes[i].legend()
    
    # Supprimer les sous-graphiques vides
    for i in range(len(numeric_cols), len(axes)):
        fig.delaxes(axes[i])
    
    plt.tight_layout()
    plt.show()
    
    # Tests de normalit√©
    print("\nüî¨ Tests de normalit√© (Shapiro-Wilk):")
    print("(p < 0.05 = distribution non normale)")
    for col in numeric_cols:
        if len(df[col]) <= 5000:  # Shapiro-Wilk limit√© √† 5000 observations
            stat, p_value = stats.shapiro(df[col])
            normality = "Normale" if p_value > 0.05 else "Non normale"
            print(f"   {col}: statistique={stat:.4f}, p-value={p_value:.6f} ‚Üí {normality}")

def outlier_analysis(df):
    """Analyse des valeurs aberrantes"""
    print("\nüîç 7. ANALYSE DES VALEURS ABERRANTES")
    print("-" * 50)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols.remove('PoorCare')
    
    outlier_summary = []
    
    # M√©thode IQR pour chaque variable
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
        n_outliers = len(outliers)
        outlier_percentage = (n_outliers / len(df)) * 100
        
        outlier_summary.append({
            'Variable': col,
            'Q1': Q1,
            'Q3': Q3,
            'IQR': IQR,
            'Limite_inf': lower_bound,
            'Limite_sup': upper_bound,
            'Nb_outliers': n_outliers,
            'Pourcentage': outlier_percentage
        })
        
        print(f"üìä {col}:")
        print(f"   Limites: [{lower_bound:.2f}, {upper_bound:.2f}]")
        print(f"   Outliers: {n_outliers} ({outlier_percentage:.2f}%)")
        if n_outliers > 0:
            print(f"   Valeurs: {sorted(outliers.tolist())}")
        print()
    
    outlier_df = pd.DataFrame(outlier_summary)
    
    # Visualisation avec boxplots
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for i, col in enumerate(numeric_cols):
        if i < len(axes):
            bp = axes[i].boxplot(df[col], patch_artist=True)
            bp['boxes'][0].set_facecolor('lightblue')
            axes[i].set_title(f'Boxplot de {col}')
            axes[i].set_ylabel(col)
            axes[i].grid(True, alpha=0.3)
    
    # Supprimer les sous-graphiques vides
    for i in range(len(numeric_cols), len(axes)):
        fig.delaxes(axes[i])
    
    plt.tight_layout()
    plt.show()
    
    return outlier_df

def correlation_analysis(df):
    """Analyse des corr√©lations"""
    print("\nüîó 8. ANALYSE DES CORR√âLATIONS")
    print("-" * 50)
    
    # Matrice de corr√©lation
    correlation_matrix = df.corr()
    
    print("üìä Matrice de corr√©lation (Pearson):")
    print(correlation_matrix.round(3))
    
    # Corr√©lations avec la variable cible
    print("\nüéØ Corr√©lations avec PoorCare:")
    target_corr = correlation_matrix['PoorCare'].drop('PoorCare').sort_values(key=abs, ascending=False)
    
    for var, corr in target_corr.items():
        strength = ""
        if abs(corr) > 0.7:
            strength = "Tr√®s forte"
        elif abs(corr) > 0.5:
            strength = "Forte"
        elif abs(corr) > 0.3:
            strength = "Mod√©r√©e"
        elif abs(corr) > 0.1:
            strength = "Faible"
        else:
            strength = "Tr√®s faible"
        
        direction = "positive" if corr > 0 else "n√©gative"
        print(f"   {var}: {corr:.3f} ({strength} {direction})")
    
    # Heatmap des corr√©lations
    plt.figure(figsize=(12, 10))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,
                square=True, fmt='.3f', cbar_kws={"shrink": .8})
    plt.title('Matrice de corr√©lation (triangle inf√©rieur)')
    plt.tight_layout()
    plt.show()
    
    # Test de significativit√© des corr√©lations
    print("\nüî¨ Tests de significativit√© des corr√©lations avec PoorCare:")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols.remove('PoorCare')
    
    for col in numeric_cols:
        corr_coef, p_value = pearsonr(df[col], df['PoorCare'])
        significance = "Significative" if p_value < 0.05 else "Non significative"
        print(f"   {col}: r={corr_coef:.3f}, p={p_value:.6f} ‚Üí {significance}")
    
    return correlation_matrix

def bivariate_analysis(df):
    """Analyse bivari√©e d√©taill√©e"""
    print("\nüìà 9. ANALYSE BIVARI√âE (VARIABLES vs PoorCare)")
    print("-" * 50)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols.remove('PoorCare')
    
    # Comparaison des distributions par classe
    n_cols = 2
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 6*n_rows))
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    
    for i, col in enumerate(numeric_cols):
        row, col_idx = i // n_cols, i % n_cols
        
        # Boxplot par classe
        df.boxplot(column=col, by='PoorCare', ax=axes[row, col_idx])
        axes[row, col_idx].set_title(f'{col} par PoorCare')
        axes[row, col_idx].set_xlabel('PoorCare')
        axes[row, col_idx].set_ylabel(col)
    
    # Supprimer les sous-graphiques vides
    for i in range(len(numeric_cols), n_rows * n_cols):
        row, col_idx = i // n_cols, i % n_cols
        fig.delaxes(axes[row, col_idx])
    
    plt.tight_layout()
    plt.show()
    
    # Tests statistiques
    print("\nüî¨ Tests statistiques (Mann-Whitney U):")
    print("(Comparaison des distributions entre PoorCare=0 et PoorCare=1)")
    
    for col in numeric_cols:
        group_0 = df[df['PoorCare'] == 0][col]
        group_1 = df[df['PoorCare'] == 1][col]
        
        statistic, p_value = stats.mannwhitneyu(group_0, group_1, alternative='two-sided')
        significance = "Significative" if p_value < 0.05 else "Non significative"
        
        median_0 = group_0.median()
        median_1 = group_1.median()
        
        print(f"   {col}:")
        print(f"      M√©diane (PoorCare=0): {median_0:.2f}")
        print(f"      M√©diane (PoorCare=1): {median_1:.2f}")
        print(f"      Statistique U: {statistic:.2f}")
        print(f"      p-value: {p_value:.6f} ‚Üí {significance}")
        print()

def advanced_visualizations(df):
    """Visualisations avanc√©es"""
    print("\nüé® 10. VISUALISATIONS AVANC√âES")
    print("-" * 50)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols.remove('PoorCare')
    
    # 1. Pairplot avec distinction par classe
    print("üìä Cr√©ation du pairplot...")
    plt.figure(figsize=(15, 12))
    
    # S√©lectionner un sous-ensemble pour la lisibilit√©
    selected_cols = numeric_cols[:4] + ['PoorCare']  # 4 premi√®res variables + target
    
    sns.pairplot(df[selected_cols], hue='PoorCare', diag_kind='hist', 
                 plot_kws={'alpha': 0.6}, diag_kws={'alpha': 0.7})
    plt.suptitle('Pairplot des principales variables', y=1.02)
    plt.show()
    
    # 2. Heatmap des moyennes par classe
    print("üìä Cr√©ation de la heatmap des moyennes...")
    
    means_by_class = df.groupby('PoorCare')[numeric_cols].mean()
    
    plt.figure(figsize=(12, 6))
    sns.heatmap(means_by_class.T, annot=True, cmap='RdYlBu_r', 
                fmt='.2f', cbar_kws={'label': 'Valeur moyenne'})
    plt.title('Moyennes des variables par classe PoorCare')
    plt.xlabel('PoorCare')
    plt.ylabel('Variables')
    plt.tight_layout()
    plt.show()
    
    # 3. Distribution des variables par classe (violin plots)
    print("üìä Cr√©ation des violin plots...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for i, col in enumerate(numeric_cols):
        if i < len(axes):
            sns.violinplot(data=df, x='PoorCare', y=col, ax=axes[i])
            axes[i].set_title(f'Distribution de {col} par PoorCare')
            axes[i].grid(True, alpha=0.3)
    
    # Supprimer les sous-graphiques vides
    for i in range(len(numeric_cols), len(axes)):
        fig.delaxes(axes[i])
    
    plt.tight_layout()
    plt.show()

def generate_report_summary(df, correlation_matrix, outlier_df):
    """G√©n√©ration du r√©sum√© du rapport"""
    print("\nüìã 11. R√âSUM√â EX√âCUTIF DE L'ANALYSE")
    print("=" * 80)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols.remove('PoorCare')
    
    print("üîç POINTS CL√âS IDENTIFI√âS:")
    print("-" * 30)
    
    # 1. Qualit√© des donn√©es
    missing_count = df.isnull().sum().sum()
    print(f"‚úÖ Qualit√© des donn√©es: {missing_count} valeurs manquantes")
    
    # 2. √âquilibrage des classes
    target_counts = df['PoorCare'].value_counts()
    ratio = target_counts.min() / target_counts.max()
    balance_status = "√âquilibr√©" if ratio > 0.5 else "D√©s√©quilibr√©" if ratio < 0.3 else "L√©g√®rement d√©s√©quilibr√©"
    print(f"‚öñÔ∏è  √âquilibrage des classes: {balance_status} (ratio: {ratio:.3f})")
    
    # 3. Variables les plus corr√©l√©es avec la cible
    target_corr = correlation_matrix['PoorCare'].drop('PoorCare')
    strongest_corr = target_corr.abs().idxmax()
    strongest_corr_value = target_corr[strongest_corr]
    print(f"üéØ Variable la plus corr√©l√©e: {strongest_corr} (r={strongest_corr_value:.3f})")
    
    # 4. Outliers
    total_outliers = outlier_df['Nb_outliers'].sum()
    outlier_percentage = (total_outliers / (len(df) * len(numeric_cols))) * 100
    print(f"üìä Outliers d√©tect√©s: {total_outliers} ({outlier_percentage:.2f}% du dataset)")
    
    # 5. Variables avec distributions non normales
    non_normal_vars = []
    for col in numeric_cols:
        if len(df[col]) <= 5000:
            stat, p_value = stats.shapiro(df[col])
            if p_value < 0.05:
                non_normal_vars.append(col)
    
    print(f"üìà Variables non normales: {len(non_normal_vars)}/{len(numeric_cols)}")
    
    print("\nüí° RECOMMANDATIONS:")
    print("-" * 20)
    
    if ratio < 0.5:
        print("‚Ä¢ Consid√©rer des techniques de r√©√©quilibrage (SMOTE, sous-√©chantillonnage)")
    
    if total_outliers > 0:
        print("‚Ä¢ Examiner et traiter les valeurs aberrantes si n√©cessaire")
    
    if len(non_normal_vars) > len(numeric_cols) / 2:
        print("‚Ä¢ Envisager des transformations (log, racine carr√©e) pour normaliser")
    
    if abs(strongest_corr_value) < 0.3:
        print("‚Ä¢ Corr√©lations faibles : explorer feature engineering ou variables additionnelles")
    
    print("‚Ä¢ Valider les r√©sultats avec validation crois√©e")
    print("‚Ä¢ Consid√©rer des mod√®les non-lin√©aires pour capturer des relations complexes")
    
    print("\n" + "=" * 80)

def main():
    """Fonction principale d'ex√©cution de l'EDA"""
    try:
        # 1. Chargement et informations de base
        df = load_and_basic_info()
        
        # 2. Analyse des valeurs manquantes
        missing_df = missing_values_analysis(df)
        
        # 3. Conversion des types de donn√©es
        df = data_types_and_conversion(df)
        
        # 4. Statistiques descriptives
        desc_stats, additional_stats = descriptive_statistics(df)
        
        # 5. Analyse de la variable cible
        target_counts, target_props = target_variable_analysis(df)
        
        # 6. Distributions des variables
        feature_distributions(df)
        
        # 7. Analyse des outliers
        outlier_df = outlier_analysis(df)
        
        # 8. Analyse des corr√©lations
        correlation_matrix = correlation_analysis(df)
        
        # 9. Analyse bivari√©e
        bivariate_analysis(df)
        
        # 10. Visualisations avanc√©es
        advanced_visualizations(df)
        
        # 11. R√©sum√© du rapport
        generate_report_summary(df, correlation_matrix, outlier_df)
        
        print("\nüéâ ANALYSE EXPLORATOIRE TERMIN√âE AVEC SUCC√àS!")
        print("üìä Toutes les visualisations et analyses ont √©t√© g√©n√©r√©es.")
        
        return df, correlation_matrix, outlier_df
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'ex√©cution: {str(e)}")
        return None, None, None

if __name__ == "__main__":
    # Installation des packages requis si n√©cessaire
    try:
        import plotly
    except ImportError:
        print("üì¶ Installation de plotly...")
        import subprocess
        subprocess.check_call(['pip', 'install', 'plotly'])
        import plotly.express as px
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import plotly.figure_factory as ff
    
    # Ex√©cution de l'analyse
    df, correlation_matrix, outlier_df = main()